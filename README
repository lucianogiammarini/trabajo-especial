http://trabajo-especial.googlecode.com/svn/trunk/

###############################################################################
(crawler.py -c 10000 -v visited.txt -i ignore.txt -o ask.xml user)

El archivo ask.xml, resultante de ejecutar crawler.py, contine las 
preguntas-respuestas obtenidas del sitio ask.fm, cada una en el siguiente formato

<questionBox user="....">
	<question author="....">....</question>
	<answer>....</answer>
</questionBox>

###############################################################################
(tokenizer.py ask.xml -o tokens.xml -t tokens.txt -l)

El modulo tokenizer.py toma este archivo como entrada y lo separa en tokens.
Un token puede ser:
	*Una palabra (secuencia de letras a..z incluyendo letras con acentos, 
	dieresis, etc, y incluyendo numeros)
	*Secuencia de simbolos (los simbolos se separan de las palabras aunque no haya un espacio)

Cada token es divido por el separador </br>

###############################################################################
(counter.py tokens.xml -o norm_count.txt)
(sort userCount.txt -nr -k3 > norm_count.sorted)

Información del corpus:

	Nro de Preguntas: 10424
	# cat ask.xml | grep -G "\<question[\> ]" | wc -l

	Nro de Respuestas: 9847
	# cat ask.xml | grep -G "\<answer\>" | wc -l

	Cantidad de Tokens: 218663
	# wc -w tokens.txt

	Longitud del lexicon: 26005
	# sort tokens.txt | uniq | wc -w
	Longitud del lexicon Minusculas: 20663
	# sort tokens_lower.txt | uniq | wc -w

	Cantidad de usuarios: 407
	# wc -l visited.txt

# sort tokens.txt | uniq -c | sort -nr > tokens.count

