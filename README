http://trabajo-especial.googlecode.com/svn/trunk/

###############################################################################
(crawler.py -c 10000 -v visited.txt -i ignore.txt -o ask.xml user)

El archivo ask.xml, resultante de ejecutar crawler.py, contine las 
preguntas-respuestas obtenidas del sitio ask.fm, cada una en el siguiente formato

<questionBox user="....">
	<question author="....">....</question>
	<answer>....</answer>
</questionBox>

###############################################################################
(tokenizer.py ask.xml -o tokens.xml -t tokens.txt -l)

El modulo tokenizer.py toma este archivo como entrada y lo separa en tokens.
Un token puede ser:
	*Una palabra (secuencia de letras a..z incluyendo letras con acentos, 
	dieresis, etc, y incluyendo numeros)
	*Secuencia de simbolos (los simbolos se separan de las palabras aunque no haya un espacio)

Cada token es divido por el separador </br>

###############################################################################
(counter.py tokens.xml -o norm_count.txt -s norm_count.sorted)
(cut -f1 norm_count.sorted | head -n 1500 > ask_dict.txt)
(cat ask_dict.txt dict.txt es_ES.txt emoticons.txt | sort | uniq > extended_dict.txt)

Información del corpus:

	Nro de Preguntas: 10462
	# cat ask.xml | grep -G "\<question[\> ]" | wc -l

	Nro de Respuestas: 9879
	# cat ask.xml | grep -G "\<answer\>" | wc -l

	Cantidad de Tokens: 221735
	# wc -w tokens.txt

	Longitud del lexicon: 20746
	# sort tokens.txt | uniq | wc -w

	Cantidad de usuarios: 407
	# wc -l visited.txt

	Cantidad de tokens en el diccionario: 2276 de 26005
	# wc.py types.txt dict.txt

	Cantidad de tokens en el diccionario extendido: 3294 de 26005
	# wc.py types.txt extended_dict.txt

# sort tokens.txt | uniq -c | sort -nr > tokens.count

